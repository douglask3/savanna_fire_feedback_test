{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<table style=\"width: 100%; border-collapse: collapse;\" border=\"0\">\n",
    "<tr>\n",
    "<td><b>Created:</b> Tuesday 7 August 2018</td>\n",
    "<td style=\"text-align: right;\"><a href=\"https://github.com/douglask3/savanna_fire_feedback_test\">github.com/douglask3/savanna_fire_feedback_test</td>\n",
    "</tr>\n",
    "</table>\n",
    "\n",
    "<div>\n",
    "<center>\n",
    "<font face=\"Times\">\n",
    "<br>\n",
    "<h1>Quantifying the uncertainity in climate potentials and mortality using Bayesian inference</h1>\n",
    "<h2>Part 2: Bayesian inference</h2>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1,* </sup>Douglas Kelley, \n",
    "<sup>1 </sup>France Gerard, \n",
    "<sup>2 </sup>Rhys Whitley, \n",
    "<sup>3 </sup>Elmar Veenendaal\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<sup>1 </sup>Centre for Ecology and Hydrology, Maclean Building, Crowmarsh Gifford, Wallingford, Oxfordshire, United Kingdom\n",
    "<br>\n",
    "<sup>2 </sup>Natural Perils Pricing, Commercial & Consumer Portfolio & Pricing, Suncorp Group, Sydney, Australia\n",
    "<br>\n",
    "<sup>3 </sup>Wageningen Univsersity, Wageningen UR\n",
    "<br>\n",
    "<br>\n",
    "<h3>Summary</h3>\n",
    "<hr>\n",
    "<p> \n",
    "This notebook aims to quantify the model parameters of a global tree cover model model. Tree Cover is calculated as a product  precipitation, tempurature  and radiation controls describing large scale climate potetials, and mortality from fire, seasonal drought, tempuature and people, and exlusion from land use modulate tree cover. This limitation model for tree cover (LimTREE) is optimized using a Bayesian Inference framework, which provides a probability distribution for the models paramters, and allows us to describe the magnitude and relative impact of controls and varaibles in terms of probabilities. Here, we cover model description and optimization.\n",
    "</p>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<i>Python code and calculations below</i>\n",
    "<br>\n",
    "</font>\n",
    "</center>\n",
    "<hr>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model description\n",
    "Tree Cover ($T$) is calculated as a product five controls. Mean annual precipitation ($MAP$), mean annual tempurature ($MAT$) and incoming shortwave radiation ($SW$) represent three liberative controls on  $T$, i.e, increases in any of the variables lead to an increase in $T$. Mortality ($M$), which combines varaibles describing fire (represented by burnt area - $F$), drought (by number of dry days, $D$) and extreme seasonal tempuratures (by mean maximum temperuature of the hottest month, $MMxT$), and land use exclusion ($E$) from fractional urban ($U$), crop ($C$) and pasture ($P$) cover are suppresive controls. Each control is expressed as a linear combination of their respective variables and represented by a simple logistic curve:\n",
    "\n",
    "\\begin{equation}\n",
    "    f(x) = \\frac{1}{1 + e^{-k\\cdot(x-x_0)}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    T &=& F_{max} \\cdot \\prod f(x) \\\\[1em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "Where $f(x)$ is the limitation imposed by control $x = MAP, MAT, SW, M, E$ and $T_{max}$ is a maximum permitted monthly burnt area used to aid our model optimization. $x_0$ is the value of control $x$ when it imposes a limitation of 50\\% on burnt area (i.e,$f(x) = 0.5$), and $k$ is the steepness of the logistic curve, equal to ¼ of the gradient at $x = x_0$. $k > 0$ for $MAP$, $MAT$ and $SW$, where $T$ increases with the control, whereas $k < 0$ for $M$ and $E$, for which $T$ decreases. Each suppresive control\n",
    "is represented by a combination of variables ($x_i$) weighted by their respective influence ($v_i$). Where possible, units are consistent across variables within each control, and as such the combined variables are normalised to maintain these units:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    x &=& \\sum v_i \\cdot x_i / \\sum v_i  \\\\[1em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    v_i = 1\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "\n",
    "This leaves 14 free parameters that need to be optimised against observations of burnt area.\n",
    "\n",
    "Now lets get this coded up in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from   io     import StringIO\n",
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "import pymc3  as pm3 \n",
    "from   pymc3.backends import SQLite\n",
    "from   scipy  import optimize\n",
    "from   theano import tensor as tt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# setup nice plotting\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# paths and parameters\n",
    "sample_pc     = 10\n",
    "nIterations   = 10000 # 20000\n",
    "nJobs         = 1    # 12\n",
    "nChains       = 1   # 10\n",
    "outPath       = \"../data/vegFracControls.csv\"\n",
    "param_outpath = '../outputs/params-negDist'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def npLog0 (x):\n",
    "    return [np.log(i) if i > 0 else 0.0000001 for i in x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2.1 Fire limitation model definition\n",
    "\n",
    "Could possibly contain this in a class object, but I'm not sure theano can instantiate the object to be used by the GPU. If I've made absolutely no sense just then, then I would leave the following as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as browser\n",
    "\n",
    "def tt_log(x):\n",
    "    x = tt.clip(x, 0.0000001, 1000000000)\n",
    "    return tt.log(x)\n",
    "\n",
    "def tt_sigmoid(x, k, x0):\n",
    "    \"\"\"\n",
    "    Sigmoid function to describe limitation using tensor\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + tt.exp(-k*(x - x0)))\n",
    "\n",
    "\n",
    "def MAP(map, drought, m_drought):\n",
    "    \"\"\"\n",
    "    Definition to describe precip: while we are just returning input for now,\n",
    "    we include climate potential functions for capability to be modified later.\n",
    "    \"\"\"\n",
    "    map = map + (1.0-drought) * (1/m_drought) * (tt.exp(-m_drought * map) - 1)\n",
    "    map = tt_log(map)\n",
    "    return map\n",
    "\n",
    "def MAT(mat, min_mat, max_mat):\n",
    "    \"\"\"\n",
    "    Definition to describe temp.\n",
    "    \"\"\"\n",
    "    mat = mat - min_mat\n",
    "    #mat = mat / (max_mat - min_mat)\n",
    "    mat = tt.clip(mat, 0.0000001, 100000)\n",
    "    mat = tt.log(mat)\n",
    "    return mat\n",
    "\n",
    "\n",
    "def SW(sw1, sw2, d):\n",
    "    \"\"\"\n",
    "    Definition to describe short wave\n",
    "    \"\"\"\n",
    "    return sw1 + d * sw2 /(1.0 + d)\n",
    "\n",
    "\n",
    "def mortality(fire, drought, maxTemp, pop_density, v_drought, v_maxTemp, v_pop,\n",
    "              p_fire, k_popden, p_drought, min_maxTemp, max_maxTemp, p_maxTemp):\n",
    "    \"\"\"\n",
    "    Definition of mortality\n",
    "    \"\"\"\n",
    "    maxTemp = maxTemp - min_maxTemp\n",
    "    maxTemp = maxTemp / (max_maxTemp - min_maxTemp)\n",
    "    maxTemp = tt.clip(maxTemp, 0.0, 1.0)\n",
    "    maxTemp = maxTemp **(p_maxTemp)\n",
    "    \n",
    "    drought = drought ** (p_drought)\n",
    "    pop_density = 1 - tt.exp(pop_density * (-1.0/ k_popden))\n",
    "    \n",
    "    fire = fire**(p_fire)\n",
    "    mort = fire  +  v_drought * drought  +  v_maxTemp * maxTemp  +  v_pop * pop_density \n",
    "\n",
    "    return mort / (1.0 + v_drought + v_maxTemp + v_pop)\n",
    "\n",
    "def exclusion(urban_area, crop_area, pasture_area, v_crop, v_pas):\n",
    "    \"\"\"\n",
    "    Definition for the measure of fire supression\n",
    "    \"\"\"\n",
    "    excl = urban_area  +  v_crop * crop_area + v_pas * pasture_area\n",
    "    \n",
    "    return excl / (1.0 + v_crop + v_pas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import data\n",
    "\n",
    "Load data and do any necessary transformation needed for the Bayesian modelling framework. We  going to load 10% of data-points. When we evalutate, that gives us 90% spare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_with_buffer(filename, line_select, **kwargs):\n",
    "    s_buf = StringIO()\n",
    "    line_select = np.sort(line_select)\n",
    "    with open(filename) as file:\n",
    "        count = -1\n",
    "        lineN = -1\n",
    "        for line in file:\n",
    "            lineN += 1\n",
    "            if lineN == 0 or lineN == line_select[count]:\n",
    "                s_buf.write(line)\n",
    "                count += 1\n",
    "                if count == len(line_select): break\n",
    "            \n",
    "    s_buf.seek(0)\n",
    "    df = pd.read_csv(s_buf,**kwargs)\n",
    "    return df\n",
    "\n",
    "def file_len(fname):\n",
    "    with open(fname) as f:\n",
    "        for i, l in enumerate(f): pass\n",
    "    return i + 1\n",
    "\n",
    "DATAPATH = os.path.expanduser(outPath)\n",
    "\n",
    "nlines      = file_len(DATAPATH)\n",
    "npoints     = round(sample_pc * nlines / 100)\n",
    "line_select = np.random.choice(range(0, nlines), npoints, False)\n",
    "fd          = load_with_buffer(DATAPATH, line_select)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a sanity check to make sure our data has imported correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def log(x, xmin = 0.00001):\n",
    "#    x[x < xmin] = xmin\n",
    "#    return np.log(x)\n",
    "#cols = [col for col in fd.columns if \"MAP_\" in col]\n",
    "#for col in cols: fd[col] = log(fd[col].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2408 entries, 0 to 2407\n",
      "Data columns (total 30 columns):\n",
      "BurntArea       2408 non-null float64\n",
      "crop            2408 non-null float64\n",
      "MADD_CMORPH     2408 non-null float64\n",
      "MADD_CRU        2408 non-null float64\n",
      "MADD_GPCC       2408 non-null float64\n",
      "MADD_MSWEP      2408 non-null float64\n",
      "MADM_CMORPH     2408 non-null float64\n",
      "MADM_CRU        2408 non-null float64\n",
      "MADM_GPCC       2408 non-null float64\n",
      "MADM_MSWEP      2408 non-null float64\n",
      "MAP_CMORPH      2408 non-null float64\n",
      "MAP_CRU         2408 non-null float64\n",
      "MAP_GPCC        2408 non-null float64\n",
      "MAP_MSWEP       2408 non-null float64\n",
      "MAT             2408 non-null float64\n",
      "MConc_CMORPH    2408 non-null float64\n",
      "MConc_CRU       2408 non-null float64\n",
      "MConc_GPCC      2408 non-null float64\n",
      "MConc_MSWEP     2408 non-null float64\n",
      "MDDM_CMORPH     2408 non-null float64\n",
      "MDDM_CRU        2408 non-null float64\n",
      "MDDM_GPCC       2408 non-null float64\n",
      "MDDM_MSWEP      2408 non-null float64\n",
      "MTWM            2408 non-null float64\n",
      "pas             2408 non-null float64\n",
      "PopDen          2408 non-null float64\n",
      "SW              2408 non-null float64\n",
      "SW.1            2408 non-null float64\n",
      "TreeCover       2408 non-null float64\n",
      "urban           2408 non-null float64\n",
      "dtypes: float64(30)\n",
      "memory usage: 564.5 KB\n",
      "None\n",
      "   BurntArea      crop  MADD_CMORPH  MADD_CRU  MADD_GPCC  MADD_MSWEP  \\\n",
      "0   0.001204  0.022631     0.779068  0.717094   0.706492    0.617865   \n",
      "1   0.002698  0.041696     0.770521  0.769779   0.653494    0.587755   \n",
      "2   0.002108  0.133132     0.740007  0.698096   0.725664    0.542395   \n",
      "3   0.004262  0.021864     0.619148  0.621305   0.448163    0.442106   \n",
      "4   0.000000  0.000000     0.941099  1.000000   0.914347    0.826162   \n",
      "\n",
      "   MADM_CMORPH  MADM_CRU  MADM_GPCC  MADM_MSWEP    ...     MDDM_CRU  \\\n",
      "0     0.624523  0.921989   0.626454    0.657141    ...     0.954781   \n",
      "1     0.630807  0.905011   0.592460    0.606172    ...     0.959477   \n",
      "2     0.313540  0.817372   0.357523    0.361618    ...     0.930684   \n",
      "3     0.528907  0.801590   0.508319    0.515498    ...     0.882952   \n",
      "4     0.899271  1.000000   0.681933    0.538752    ...     1.000000   \n",
      "\n",
      "   MDDM_GPCC  MDDM_MSWEP       MTWM       pas    PopDen         SW       SW.1  \\\n",
      "0   0.932554    0.862084  29.764286  0.255358  0.700120  355.88248  156.99090   \n",
      "1   0.849391    0.814032  29.300000  0.221954  6.208023  355.88248  150.50359   \n",
      "2   0.875005    0.714184  29.985714  0.448686  6.049409  355.88248  149.08888   \n",
      "3   0.691974    0.742262  28.407143  0.001146  6.634046  355.88248  157.89609   \n",
      "4   1.000000    0.999424  32.600000  0.117650  3.569709  355.88248  220.64978   \n",
      "\n",
      "   TreeCover     urban  \n",
      "0   0.104286  0.000000  \n",
      "1   0.136935  0.000000  \n",
      "2   0.105417  0.000000  \n",
      "3   0.461875  0.000000  \n",
      "4   0.000000  0.000027  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "BurntArea         0.000000\n",
      "crop              0.000000\n",
      "MADD_CMORPH       0.084662\n",
      "MADD_CRU          0.086009\n",
      "MADD_GPCC         0.059622\n",
      "MADD_MSWEP        0.000098\n",
      "MADM_CMORPH       0.162663\n",
      "MADM_CRU          0.165354\n",
      "MADM_GPCC         0.000000\n",
      "MADM_MSWEP        0.112024\n",
      "MAP_CMORPH        1.088500\n",
      "MAP_CRU           0.000000\n",
      "MAP_GPCC          0.000035\n",
      "MAP_MSWEP         0.026761\n",
      "MAT              -4.866071\n",
      "MConc_CMORPH      0.072306\n",
      "MConc_CRU         0.004648\n",
      "MConc_GPCC        0.067273\n",
      "MConc_MSWEP       0.054917\n",
      "MDDM_CMORPH       0.148528\n",
      "MDDM_CRU          0.262068\n",
      "MDDM_GPCC         0.113713\n",
      "MDDM_MSWEP        0.000000\n",
      "MTWM              3.728571\n",
      "pas               0.000000\n",
      "PopDen            0.000000\n",
      "SW              355.882480\n",
      "SW.1             35.488420\n",
      "TreeCover         0.000000\n",
      "urban             0.000000\n",
      "dtype: float64\n",
      "BurntArea          0.931444\n",
      "crop               0.688072\n",
      "MADD_CMORPH        0.995513\n",
      "MADD_CRU           1.000000\n",
      "MADD_GPCC          1.000000\n",
      "MADD_MSWEP         0.999853\n",
      "MADM_CMORPH        1.000000\n",
      "MADM_CRU           1.000000\n",
      "MADM_GPCC          1.000000\n",
      "MADM_MSWEP         1.000000\n",
      "MAP_CMORPH      6955.262000\n",
      "MAP_CRU         7731.857000\n",
      "MAP_GPCC        7083.018000\n",
      "MAP_MSWEP       7057.908700\n",
      "MAT               30.802977\n",
      "MConc_CMORPH       1.000000\n",
      "MConc_CRU          1.000000\n",
      "MConc_GPCC         1.000000\n",
      "MConc_MSWEP        1.000000\n",
      "MDDM_CMORPH        1.000000\n",
      "MDDM_CRU           1.000000\n",
      "MDDM_GPCC          1.000000\n",
      "MDDM_MSWEP         1.000000\n",
      "MTWM              38.171430\n",
      "pas                0.831616\n",
      "PopDen          2574.803500\n",
      "SW               415.134120\n",
      "SW.1             353.319370\n",
      "TreeCover          0.796161\n",
      "urban              0.438081\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(fd.info())\n",
    "print(fd.head())\n",
    "print(fd.min())\n",
    "print(fd.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Baysian framework\n",
    "\n",
    "A simple explanation of Baye's law is:\n",
    "\n",
    "\\begin{equation}\n",
    "    P(\\beta|X) \\propto P(\\beta)\\cdot P(X|\\beta)\n",
    "\\end{equation}\n",
    "\n",
    "where $X$ is our data (observations of some arbitrary system), and $\\beta$ our set of unexplained parameters that describe the reponse of our _proposed understanding_ of this system as it varies with $X$.\n",
    "\n",
    "### 2.3.1 Prior definitions\n",
    "Because I have no idea what the uncertainty on the hyper parameters should look like (beyond $\\beta> 0$), I've set them all as wide, familiar distrubtions, spread generously beyound what is realistic plausable. Our full prior looks like this:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(x_0) &=& \\mathcal{N}(0.5, 0.5) \\\\\n",
    "    P(k_x) &=& \\mathcal{exp}(0.1) \\\\\n",
    "    P(T_{max}) = P(p_{popden}) = P(v_i) &=& \\mathcal{exp}(1) \\\\\n",
    "    P(\\sigma) &=& \\mathcal{N_{1/2}}(1) \\\\[1.5em]\n",
    "\\end{eqnarray}\n",
    "\n",
    "So our full set of piors looks something like this:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(\\beta) &=& \\prod_{x=1}^{4}\\mathcal{S}(P(x_0),P(k_x), P(v_x)) \\cdot P(D_{max}) \\cdot P(\\sigma) \\\\[1.5em]  \n",
    "\\end{eqnarray}\n",
    "\n",
    "This is loosly describes a (albeit slightly skewed) Normal distribution, so we can approximate our error ($\\sigma$) with a normal distibution. \n",
    "\n",
    "Back to the code.., `pymc3` is quite funky in that it allows me to create an empty `Model()` object and just add things to it as I need them using a `with` statement. I've called our Bayesian model `tree_error` as that is what we are trying to Quantify.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "nChains = nChains * nJobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutLastX(varname, mcmc_traces, ncut = 50):\n",
    "    vals = mcmc_traces.get_values(varname)\n",
    "    def subcut(vals, r, ncut = 50):\n",
    "        cut_np = (r+1) * round(len(vals)/nChains)\n",
    "        ncut = round(len(vals) * ncut / (nChains *100))\n",
    "        return vals[(cut_np - ncut):cut_np]\n",
    "    vals = [subcut(vals, r) for r in range(nChains)]\n",
    "    return np.array(vals).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pr_droughtVar(pr_dataset, drough_var):\n",
    "    with pm3.Model() as tree_error:\n",
    "\n",
    "    # first for the sigmoids  \n",
    "        MAP_x0         = pm3.Normal('MAP_x0'       ,  5.0, 1.0)\n",
    "        MAP_k          = pm3.Exponential('MAP_k'        ,  1.0)\n",
    "\n",
    "        MAT_x0         = pm3.Normal('MAT_x0'   ,  0.0, 1.0)\n",
    "        MAT_k          = pm3.Exponential('MAT_k'    ,  1.0)\n",
    "\n",
    "        #SW_x0          = pm3.Exponential('SW_x0'   ,  1/300.0)\n",
    "        #SW_k           = pm3.Exponential('SW_k'    ,        1.5)\n",
    "\n",
    "        mort_x0        = pm3.Normal('mort_x0', 0.5, 0.5)\n",
    "        mort_k         = pm3.Exponential('mort_k' , 1.0)\n",
    "\n",
    "        ex_x0          = pm3.Normal('ex_x0', 0.5, 0.5)\n",
    "        ex_k           = pm3.Exponential('ex_k' , 1.0)\n",
    "\n",
    "        max_T          = pm3.Uniform('max_T'         ,     0.0,   0.8)\n",
    "        #pow_f          = pm3.Uniform('pow_f'         ,     0.0,    1.0)\n",
    "    # now for the hyper-parameters that describe the independent fire condition covariates\n",
    "\n",
    "        #cNpp = pm3.Uniform('cNPP', 0.0, 10)\n",
    "        m_drought   = pm3.Exponential('m_drought', 1.0)\n",
    "        v_drought   = pm3.Exponential('v_drought', 1.0)\n",
    "        v_maxTemp   = pm3.Exponential('v_maxTemp', 1.0)\n",
    "        v_popDen    = pm3.Exponential('v_popDen' , 1.0)\n",
    "        v_crop      = pm3.Exponential('v_crop'   , 1.0)\n",
    "        v_pas       = pm3.Exponential('v_pas'    , 1.0)\n",
    "\n",
    "        #trans_d    = pm3.Exponential('trans_d'  , 1.0)\n",
    "        p_fire      = pm3.Exponential('p_fire', 1.0)\n",
    "        k_popden    = pm3.Exponential('k_popden' , 10.0)\n",
    "\n",
    "        p_drought   = pm3.Exponential('p_drought', 1.0)\n",
    "        min_maxTemp = pm3.Normal     ('min_maxTemp', 20.0, 31.34)\n",
    "        max_maxTemp = pm3.Normal     ('max_maxTemp', 30.0, 31.34)\n",
    "        min_mat     = pm3.Normal     ('min_mat', 0.0, 3.0)\n",
    "        max_mat     = pm3.Normal     ('max_mat', 15.0, 5.0)\n",
    "        p_maxTemp   = pm3.Exponential('p_maxTemp', 1.0)\n",
    "    # describe the standard deviation in the error term\n",
    "        sigma       = pm3.HalfNormal('sigma', sd=1)\n",
    "\n",
    "\n",
    "        # transform hyper-covariates \n",
    "        MAP_var = 'MAP_' + pr_dataset\n",
    "        drght_var = drough_var + '_' + pr_dataset\n",
    "        f_precip = MAP(fd[MAP_var].values, fd[drght_var].values, m_drought)\n",
    "        f_temp   = MAT(fd[\"MAT\"].values, min_mat, max_mat)\n",
    "        #f_sw     = SW (fd[\"SW\"].values, fd[\"SW.1\"].values, trans_d)\n",
    "        f_mort   = mortality(fd[\"BurntArea\"].values, fd[drght_var].values, fd[\"MTWM\"].values, fd[\"PopDen\"].values, \n",
    "                             v_drought, v_maxTemp, v_popDen, p_fire, k_popden, p_drought, min_maxTemp, max_maxTemp, p_maxTemp)\n",
    "        f_exc    = exclusion(fd[\"urban\"].values, fd[\"crop\"].values, fd[\"pas\"].values, v_crop, v_pas)\n",
    "\n",
    "\n",
    "        # Tree cover is assumed to be the product of the 4 sigmoid\n",
    "\n",
    "        prediction = max_T * np.product([tt_sigmoid(f_precip, MAP_k, MAP_x0),\n",
    "                                 tt_sigmoid(f_temp, MAT_k, MAT_x0),\n",
    "                                 #tt_sigmoid(f_sw, SW_k, SW_x0),\n",
    "                                 tt_sigmoid(f_mort, - mort_k, mort_x0),\n",
    "                                 tt_sigmoid(f_exc, - ex_k, ex_x0)])\n",
    "\n",
    "        prediction = tt.clip(prediction, -100, 0.8)              \n",
    "        # calculate the error between observed and predicted burnt area\n",
    "        error = pm3.Normal('error', mu = prediction, sd = sigma, observed=fd['TreeCover'].values)\n",
    "\n",
    "         # set the step-method (criteria algorithm for moving around information space)\n",
    "        step = pm3.Metropolis()\n",
    "\n",
    "        # do the sampling\n",
    "        mcmc_traces = pm3.sample(nIterations, step=step, njobs= nJobs, chains = nChains) \n",
    "    \n",
    "    #pm3.traceplot(mcmc_traces);\n",
    "    varnames = mcmc_traces.varnames\n",
    "    \n",
    "    vals = [cutLastX(i, mcmc_traces) for i in varnames]\n",
    "\n",
    "    vals = pd.DataFrame(np.array(vals).T, columns=varnames)\n",
    "    out_fname = param_outpath + '_' + pr_dataset + '_' + drough_var + '.csv'\n",
    "    vals.to_csv(out_fname, index=False)\n",
    "    #plt.plot(vals[\"MAP_x0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:15<00:00, 15.54it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:08<00:00, 15.70it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:01<00:00, 15.87it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:57<00:00, 15.97it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:10<00:00, 15.67it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:27<00:00, 15.27it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:07<00:00, 15.72it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:15<00:00, 15.55it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [11:11<00:00, 15.63it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:47<00:00, 16.21it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:50<00:00, 16.14it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:56<00:00, 15.99it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:50<00:00, 16.13it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:55<00:00, 16.03it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:52<00:00, 16.09it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n",
      "Sequential sampling (1 chains in 1 job)\n",
      "CompoundStep\n",
      ">Metropolis: [sigma_log__]\n",
      ">Metropolis: [p_maxTemp_log__]\n",
      ">Metropolis: [max_mat]\n",
      ">Metropolis: [min_mat]\n",
      ">Metropolis: [max_maxTemp]\n",
      ">Metropolis: [min_maxTemp]\n",
      ">Metropolis: [p_drought_log__]\n",
      ">Metropolis: [k_popden_log__]\n",
      ">Metropolis: [p_fire_log__]\n",
      ">Metropolis: [v_pas_log__]\n",
      ">Metropolis: [v_crop_log__]\n",
      ">Metropolis: [v_popDen_log__]\n",
      ">Metropolis: [v_maxTemp_log__]\n",
      ">Metropolis: [v_drought_log__]\n",
      ">Metropolis: [m_drought_log__]\n",
      ">Metropolis: [max_T_interval__]\n",
      ">Metropolis: [ex_k_log__]\n",
      ">Metropolis: [ex_x0]\n",
      ">Metropolis: [mort_k_log__]\n",
      ">Metropolis: [mort_x0]\n",
      ">Metropolis: [MAT_k_log__]\n",
      ">Metropolis: [MAT_x0]\n",
      ">Metropolis: [MAP_k_log__]\n",
      ">Metropolis: [MAP_x0]\n",
      "100%|██████████| 10500/10500 [10:48<00:00, 16.19it/s]\n",
      "Only one chain was sampled, this makes it impossible to run some convergence checks\n"
     ]
    }
   ],
   "source": [
    "for pr_d in ['MSWEP', 'GPCC', 'CRU', 'CMORPH']: \n",
    "    for dr_v in ['MADD', 'MADM', 'MConc', 'MDDM']:\n",
    "        run_pr_droughtVar(pr_d, dr_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Likelihood definition\n",
    "\n",
    "For the sake of simplicity (and because I don't really know any better), we define the model error as normally distributed (i.i.d.) although it most likely isn't. We could make this more complicated later by defining the error as heteroscedastic, but I wouldn't bother with that until we have some idea of the convergence. We're describing the error (observations minus model predictions) as follows:\n",
    "\n",
    "\\begin{eqnarray}\n",
    "    P(X|\\beta) &=& \\mathcal{N}(F_{burn}, \\sigma) \\\\[1em]\n",
    "    \\mathcal{N}(F_{burn}, \\sigma) &=& \\frac{N}{\\sigma\\sqrt{2\\pi}}\\exp\\left\\{\\sum_{i=1}^{N}\\left(\\frac{y_i - F_{burn, i}}{\\sigma_i}\\right)^2\\right\\}\n",
    "\\end{eqnarray}\n",
    "\n",
    "where $y_i$ is a set of observations we're attempting to optimise on. Below is the code that describes the above:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3 Posterior sampling\n",
    "\n",
    "Because it is nigh impossible to determine the posterior solution analytically we will instead sample the information space to **infer** the posterior solutions for each of the model parameters. In this case we are using a Metropolis-Hasting step MCMC.\n",
    "\n",
    "I've tried using No-U-Turn (NUTS) sampling (which is the new kid on the block), but there are issues with it's current implementation in pymc3 (see github repo issues). Can use it once problems are ironed out - but TBH it doesn't matter if we're getting a reasonable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mcmc_traces' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-28592dc2cc8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpm3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraceplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmcmc_traces\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'mcmc_traces' is not defined"
     ]
    }
   ],
   "source": [
    "pm3.traceplot(mcmc_traces);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iterations at the start are just letting the optimization settle. So we will only sample to last 5% of iterations for futher analysis. Here, exporting to netcdf for others to do their own analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cutLastX(varname, ncut = 50):\n",
    "    vals = mcmc_traces.get_values(varname)\n",
    "    def subcut(vals, r, ncut = 50):\n",
    "        cut_np = (r+1) * round(len(vals)/nChains)\n",
    "        ncut = round(len(vals) * ncut / (nChains *100))\n",
    "        return vals[(cut_np - ncut):cut_np]\n",
    "    vals = [subcut(vals, r) for r in range(nChains)]\n",
    "    return np.array(vals).flatten()\n",
    "\n",
    "varnames = mcmc_traces.varnames\n",
    "\n",
    "vals = [cutLastX(i) for i in varnames]\n",
    "\n",
    "vals = pd.DataFrame(np.array(vals).T, columns=varnames)\n",
    "vals.to_csv(param_outpath, index=False)\n",
    "\n",
    "vals.head()\n",
    "plt.plot(vals[\"mort_x0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb cimport set_trace as browser\n",
    "def cutLastX(varname, ncut = 50):\n",
    "    vals = mcmc_traces.get_values(varname)\n",
    "    browser()\n",
    "    def subcut(vals, r, ncut = 50):\n",
    "        cut_np = (r+1) * round(len(vals)/nChains)\n",
    "        ncut = round(len(vals) * ncut / (nChains *100))\n",
    "        return vals[(cut_np - ncut):cut_np]\n",
    "    vals = [subcut(vals, r) for r in range(nChains)]\n",
    "    np.array(vals).flatten()\n",
    "\n",
    "varnames = mcmc_traces.varnames\n",
    "\n",
    "vals = [cutLastX(i) for i in varnames]\n",
    "vals = pd.DataFrame(np.array(vals).T, columns=varnames)\n",
    "vals.to_csv(param_outpath, index=False)\n",
    "\n",
    "vals.head()\n",
    "plt.plot(vals[\"mort_x0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let look at the pdf of the last 5% of iterations for each parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "variables2Plot = ['fuel_x0'       , 'fuel_k',\n",
    "                  'moisture_x0'   , 'moisture_k',\n",
    "                  'igntions_x0'   , 'igntions_k',\n",
    "                  'suppression_x0', 'suppression_k',\n",
    "                  'fuel_pw'       , 'fuel_pg',\n",
    "                  'cM'            , 'cMT',\n",
    "                  'cC'            ,\n",
    "                  'cP1'           , 'cP2', \n",
    "                  'cD1'           , 'cD2', 'max_f']\n",
    "\n",
    "nvar = len(variables2Plot)\n",
    "npcol = 4\n",
    "nprow = np.ceil(nvar / npcol)\n",
    "\n",
    "plt.figure(figsize=(20,5 * nprow))\n",
    "def plotVar(var1, pn):\n",
    "    plt.subplot(npcol, nprow, pn)\n",
    "    param = vals[var1]\n",
    "    \n",
    "    hist, bins = np.histogram(param, bins=50)\n",
    "    hist = 100.0 * hist / np.sum(hist)\n",
    "    bins = bins[1:] - np.diff(bins)/2\n",
    "    plt.plot(bins, hist)\n",
    "    plt.xlabel(var1)\n",
    "    \n",
    "pn = 0\n",
    "for i in variables2Plot:\n",
    "    pn = pn + 1\n",
    "    plotVar(i, pn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what to the sigmoids look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdb import set_trace as browser\n",
    "\n",
    "def pltVsFire(x, xlab, pnt = 'o', *args, **kw):\n",
    "    plt.plot(x, fd.fire, pnt, alpha = 0.03, *args, **kw)\n",
    "    plt.xlabel(xlab)\n",
    "    \n",
    "def np_sigmoid(x, k, x0):\n",
    "    \"\"\"\n",
    "    Sigmoid function to describe limitation using tensor\n",
    "    \"\"\"\n",
    "    return 1.0/(1.0 + np.exp(-k*(x - x0)))\n",
    "\n",
    "def returnSigmoid(x, k, x0):\n",
    "    return np_sigmoid(x, k, x0)\n",
    "    \n",
    "def meanParam(x, x0, k, kmult = 1.0):\n",
    "    x0 = np.mean(vals[x0])\n",
    "    k  = np.mean(vals[k]) * kmult\n",
    "\n",
    "    return returnSigmoid(x, k, x0)\n",
    "\n",
    "def randomParam(x, x0, k, kmult = 1.0, size = 100):\n",
    "    ps = np.random.choice(vals.shape[0], size = size, replace = False)\n",
    "    return [returnSigmoid(x, vals[k][i] * kmult, vals[x0][i]) for i in ps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "####################\n",
    "## Fuel           ##\n",
    "####################\n",
    "plt.subplot(2, 2, 1)\n",
    "## scatter plot\n",
    "fp = np.mean(vals['fuel_pw'])\n",
    "fg = np.mean(vals['fuel_pg'])\n",
    "cNPP = 1.0#np.mean(vals['cNPP'])\n",
    "\n",
    "f_fuel = fuel_load(fd[\"vegCover\"].values, fd[\"alphaMax\"].values, fp, fg)\n",
    "print(f_fuel.max())\n",
    "pltVsFire(f_fuel, \"NPP (g/m$^2$)\", 'go')\n",
    "\n",
    "## Line of best fit\n",
    "#Fuel = np.arange(-6, fd.fuel.max(), 0.01)\n",
    "Fuel = np.arange(0, f_fuel.max(), 0.01)\n",
    "r_fuel = randomParam(Fuel, 'fuel_x0', 'fuel_k')\n",
    "#NPP = np.exp(Fuel)\n",
    "for r in r_fuel: plt.plot(Fuel, r, 'k', alpha=.01)\n",
    "\n",
    "## cfg plot\n",
    "#plt.xscale('log')\n",
    "#plt.xlim([0.5, fd.NPP.max()])\n",
    "\n",
    "####################\n",
    "## Moisture       ##\n",
    "####################\n",
    "plt.subplot(2, 2, 2)\n",
    "## scatter plot\n",
    "Fmax = np.mean(vals['max_f'])\n",
    "cM = np.mean(vals['cM'])\n",
    "cMT = np.mean(vals['cMT'])\n",
    "f_moisture = moisture(fd[\"alpha\"].values, fd[\"emc\"].values, fd[\"treecover\"].values, cM, cMT)\n",
    "pltVsFire(f_moisture , \"Moisture = $\\\\alpha$ + M $\\cdot$ EMC\",'bo')\n",
    "\n",
    "## Line of best fit\n",
    "mst = np.arange(0.0, f_moisture.max(), 0.05)\n",
    "r_moisture = randomParam(mst, 'moisture_x0', 'moisture_k', -1.0)\n",
    "for r in r_moisture: plt.plot(mst, r, 'k', alpha=.01)\n",
    "\n",
    "    \n",
    "plt.ylim(0, 1)\n",
    "\n",
    "####################\n",
    "## Igntions       ##\n",
    "####################\n",
    "plt.subplot(2, 2, 3)\n",
    "## scatter plot \n",
    "cP  = np.mean(vals['cP1'])\n",
    "cC  = np.mean(vals['cC' ])\n",
    "cD1 = np.mean(vals['cD1'])\n",
    "igniteMax = 10\n",
    "\n",
    "f_ignite = ignition(fd[\"lightning_ignitions\"].values, \\\n",
    "                    fd[\"cropland\"].values, \\\n",
    "                    fd[\"pasture\"].values, \\\n",
    "                    fd[\"population_density\"].values, \\\n",
    "                    cC, cP, cD1)\n",
    "#f_ignite = np.exp(f_ignite)\n",
    "pltVsFire(f_ignite, \"Ignitions events = Lightn + P $\\cdot$ Pop Dens + D1 $\\cdot$ Pasture\")\n",
    "\n",
    "## Line of best fit\n",
    "Ignite = np.arange(0.0, igniteMax, 0.1)\n",
    "#Ignite = np.exp(Ignite)\n",
    "#=Ignite = np.arange(0.0, 10000, 1)\n",
    "r_Ignite = randomParam(Ignite, 'igntions_x0', 'igntions_k')\n",
    "#yay = returnSigmoid(Ignite, 10, 1.0)\n",
    "#Ignite = np.exp(Ignite)\n",
    "for r in r_Ignite: plt.plot(Ignite, r, 'k', alpha=.01)\n",
    "\n",
    "#plt.plot(Ignite, yay, 'red')\n",
    "\n",
    "    \n",
    "## cfg plot\n",
    "#plt.xscale('log')\n",
    "plt.xlim(0, igniteMax)\n",
    "\n",
    "####################\n",
    "## Suppression    ##\n",
    "####################\n",
    "plt.subplot(2, 2, 4)\n",
    "#scatter plot\n",
    "cP2 = np.mean(vals['cP2'])\n",
    "cD2 = np.mean(vals['cD2'])\n",
    "maxF = np.mean(vals['max_f'])\n",
    "f_suppression = maxF * supression(fd[\"cropland\"].values, \\\n",
    "                           fd[\"cropland\"].values, \\\n",
    "                           fd[\"population_density\"].values, \\\n",
    "                           cP2,\n",
    "                           cD2)\n",
    "\n",
    "pltVsFire(f_suppression, \"Suppression = Cropland + D2 $\\cdot$ Pop den\")\n",
    "\n",
    "# Line of best fit\n",
    "Suppress = np.arange(0, 100, 0.01)\n",
    "r_suppression = randomParam(Suppress, 'suppression_x0', 'suppression_k', -1.0)\n",
    "for r in r_suppression: plt.plot(Suppress, r * Fmax, 'k', alpha=.01)\n",
    "\n",
    "plt.xlim(0, 100)\n",
    "plt.ylim(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
